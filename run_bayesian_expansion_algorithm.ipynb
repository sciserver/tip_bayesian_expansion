{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4549aec3-74fb-414d-bb6a-23f7b56505a5",
   "metadata": {},
   "source": [
    "## Bayesian Expansion Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00586482-42d6-4379-93ec-23bb84cd90a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T14:01:12.837558Z",
     "iopub.status.busy": "2023-09-14T14:01:12.836687Z",
     "iopub.status.idle": "2023-09-14T14:01:16.885367Z",
     "shell.execute_reply": "2023-09-14T14:01:16.882893Z",
     "shell.execute_reply.started": "2023-09-14T14:01:12.837361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import helpers\n",
    "from mssqldb import MSSQLDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs on a SqlServer database on SciServer.\n",
    "\n",
    "If you want use something different. Roll your own database class and have \n",
    "it implement the same methods as the MSSQLDatabase class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e6685f-023d-46f7-9ef1-1bcd835ed82d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T14:01:16.898789Z",
     "iopub.status.busy": "2023-09-14T14:01:16.898023Z",
     "iopub.status.idle": "2023-09-14T14:01:16.985820Z",
     "shell.execute_reply": "2023-09-14T14:01:16.983498Z",
     "shell.execute_reply.started": "2023-09-14T14:01:16.898730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = MSSQLDatabase.from_file(\n",
    "    \"/home/idies/workspace/Storage/ryanhausen/persistent/tip/tip.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ffb85-33ba-4af6-9855-bdbadaebcc6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Database setup\n",
    "\n",
    "First build the database tables that will be used to store the results and intermediate results\n",
    "\n",
    "Setting `danger=True` will drop the results tables, so only do that if you want to start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716b2fa4-e671-4b5c-b0a8-6c0cb48d139d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-28T16:01:43.682391Z",
     "iopub.status.busy": "2023-08-28T16:01:43.681444Z",
     "iopub.status.idle": "2023-08-28T16:01:46.549952Z",
     "shell.execute_reply": "2023-08-28T16:01:46.546909Z",
     "shell.execute_reply.started": "2023-08-28T16:01:43.682325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.execute_update(helpers.get_sql(\"sql/DDL.pysql\", danger=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210636ce-2b75-484e-b8b9-da09c2df833f",
   "metadata": {},
   "source": [
    "### Run the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacd85f-9bc6-4da5-bb22-9fa2a0ac7ca6",
   "metadata": {},
   "source": [
    "1. Run the sql to fill the tables with the authors to run the algorithm on\n",
    "2. Build the sparse matrix\n",
    "3. Run the algorithm\n",
    "4. Save the results back to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e521a1d6-40cb-41a1-b428-eff969309e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T14:01:19.257035Z",
     "iopub.status.busy": "2023-09-14T14:01:19.255722Z",
     "iopub.status.idle": "2023-09-14T14:01:19.322302Z",
     "shell.execute_reply": "2023-09-14T14:01:19.320101Z",
     "shell.execute_reply.started": "2023-09-14T14:01:19.256914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/11233293/2691018\n",
    "def setup_logger(name:str, log_file:str, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "\n",
    "    handler = logging.FileHandler(log_file)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def update_year(\n",
    "    year:int,\n",
    "    db:MSSQLDatabase,\n",
    "    scopus_version:int,\n",
    "    run_id:int,\n",
    "    threshold:float,\n",
    "    prior_alpha:float,\n",
    "    prior_beta:float,\n",
    "    n_workers:int=os.cpu_count(),\n",
    ") -> None:\n",
    "\n",
    "    logger = setup_logger(\n",
    "        f\"bayesian_expansion_{year}\",\n",
    "        f\"./logs/{run_id}_{year}.log\",\n",
    "    )\n",
    "\n",
    "    logger.info(f\"run_id={run_id}\")\n",
    "\n",
    "    if n_workers < 1 or type(n_workers)!=int:\n",
    "        logger.fatal(f\"n_workers needs to be an integer >=1, actual {n_workers}\")\n",
    "        raise ValueError(\"n_workers needs to be an integer >=1\")\n",
    "\n",
    "    with helpers.LogTime(logger, \"-----Start Run-----\"):\n",
    "        with helpers.LogTime(logger, f\"building eid/auid tables for {year}\"):\n",
    "            db.execute_update(helpers.get_sql(\n",
    "                \"sql/update_year.pysql\",\n",
    "                year=year,\n",
    "                threshold=threshold,\n",
    "                run_id=run_id,\n",
    "                scopus_version=scopus_version\n",
    "            ))\n",
    "\n",
    "        # Has columns:\n",
    "        # auid:         int\n",
    "        # is_seed:      int {0, 1}\n",
    "        # intial_score: float\n",
    "        with helpers.LogTime(logger, \"getting authors\"):\n",
    "            input_auids = db.execute_query(helpers.get_sql(\n",
    "                \"sql/get_authors.pysql\",\n",
    "                year=year,\n",
    "                run_id=run_id,\n",
    "            )).astype(dict(\n",
    "                auid = np.int32,\n",
    "                is_seed = np.uint8,\n",
    "                initial_score = np.float32,\n",
    "            ))\n",
    "\n",
    "        # We want to sort by:\n",
    "        # is_seed: so that there are continguous areas of labeled and unlabeled data\n",
    "        input_auids.sort_values(\n",
    "            by=[\"is_seed\",\"auid\"],\n",
    "            ascending=[False, True],\n",
    "            ignore_index=True,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # We zip the auids with the index values to match every auid with an integer\n",
    "        # that will be it's row in the afjacency matrix\n",
    "        auids_idxs = zip(\n",
    "            input_auids[\"auid\"].values,\n",
    "            input_auids.index.values.astype(np.int32),\n",
    "        )\n",
    "\n",
    "\n",
    "        auid_idx_map = {auid:idx for auid, idx in auids_idxs}\n",
    "        transform_f = functools.partial(helpers.transform_row_to_idx_pairs, auid_idx_map)\n",
    "\n",
    "\n",
    "        # [n,]\n",
    "        score_vector = input_auids[\"initial_score\"].values\n",
    "\n",
    "        # this is the index that separates labeled and unlabeled data\n",
    "        labeled_idx = np.argmin(score_vector)\n",
    "        logger.info(f\"the split index is {labeled_idx}\")\n",
    "\n",
    "\n",
    "        # Has columns\n",
    "        # eid:   int\n",
    "        # auids: str (comma separated auids per eid)\n",
    "        with helpers.LogTime(logger, \"getting eids with auids\"):\n",
    "            input_eid_auids = db.execute_query(helpers.get_sql(\n",
    "                \"sql/get_authors_per_eid.pysql\",\n",
    "                year=year,\n",
    "                run_id=run_id,\n",
    "                scopus_version=scopus_version,\n",
    "            ))\n",
    "\n",
    "        # array of strings containing comma separated auids per eid\n",
    "        csv_auids_per_eid = input_eid_auids[\"auids\"].values.flatten()\n",
    "\n",
    "        del input_eid_auids\n",
    "\n",
    "        with helpers.LogTime(logger, \"building adjacency matrix\"):\n",
    "            if n_workers==1:\n",
    "                logger.info(\"running in serial\")\n",
    "                adjacency_matrix = helpers.arr_to_matrix(\n",
    "                    transform_f,\n",
    "                    len(score_vector),\n",
    "                    labeled_idx,\n",
    "                    csv_auids_per_eid,\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"running in parallel with {n_workers} workers\")\n",
    "                map_f = functools.partial(\n",
    "                    helpers.arr_to_matrix,\n",
    "                    transform_f,\n",
    "                    len(score_vector),\n",
    "                    labeled_idx,\n",
    "                )\n",
    "                with mp.Pool(n_workers) as p:\n",
    "                    matricies = p.map(\n",
    "                        map_f,\n",
    "                        np.array_split(csv_auids_per_eid, n_workers)\n",
    "                    )\n",
    "\n",
    "                adjacency_matrix = sum(matricies)\n",
    "\n",
    "\n",
    "        with helpers.LogTime(logger, \"Computing new labels\"):\n",
    "            score_vector = helpers.compute(\n",
    "                score_vector,\n",
    "                adjacency_matrix,\n",
    "                labeled_idx,\n",
    "                prior_alpha,\n",
    "                prior_beta,\n",
    "                max_iter=100,\n",
    "                logger=logger,\n",
    "            )\n",
    "\n",
    "        input_auids[\"final_score\"] = score_vector\n",
    "\n",
    "        with helpers.LogTime(logger, \"Deleting adj matrix/score vector\"):\n",
    "            del adjacency_matrix\n",
    "            del score_vector\n",
    "\n",
    "        # push the new scores back to the db\n",
    "        with helpers.LogTime(logger, \"Converting to records\"):\n",
    "            # the sql dtypes are:\n",
    "            # auid          INT\n",
    "            # is_seed       TINYINT\n",
    "            # initial_score FLOAT(24)\n",
    "            # final_score   FLOAT(24)\n",
    "            dtypes = dict(\n",
    "                auid=\"<i4\",\n",
    "                is_seed=\"<u1\",\n",
    "                initial_score=\"<f4\",\n",
    "                final_score=\"<f4\",\n",
    "            )\n",
    "\n",
    "\n",
    "            input_auids.astype(dtypes).to_records(\n",
    "                index=False,\n",
    "                column_dtypes=dtypes\n",
    "            ).tofile(\n",
    "                f\"/home/idies/workspace/showusthedata/tip/ryan/bulk_tmp_bayes_{run_id}_{year}.bin\"\n",
    "            )\n",
    "\n",
    "        with helpers.LogTime(logger, \"Bulk inserting records\"):\n",
    "\n",
    "            db.execute_update(helpers.get_sql(\"sql/tmp_update_DDL.pysql\"))\n",
    "            db.execute_update(helpers.get_sql(\n",
    "                \"sql/update_bulk_insert.pysql\",\n",
    "                year=year,\n",
    "                run_id=run_id,\n",
    "            ))\n",
    "\n",
    "        with helpers.LogTime(logger, \"Merging updates in database\"):\n",
    "            db.execute_update(helpers.get_sql(\n",
    "                \"sql/merge_results.pysql\",\n",
    "                year=year,\n",
    "                run_id=run_id,\n",
    "            ))\n",
    "\n",
    "\n",
    "def get_new_run_id(metadata:dict) -> int:\n",
    "    \"\"\"\n",
    "    This function retrieves the first value of the first row from the result of a SQL query.\n",
    "\n",
    "    The SQL query is defined in the file \"sql/get_authors.pysql\", and it uses the provided metadata\n",
    "    as parameters. The metadata is converted to a JSON string before being passed to the SQL query.\n",
    "\n",
    "    The result of the query is expected to be a pandas DataFrame, and this function returns the first\n",
    "    value of the first row from this DataFrame.\n",
    "\n",
    "    Args:\n",
    "        metadata (dict): A dictionary containing metadata that will be passed as parameters to the SQL query.\n",
    "\n",
    "    Returns:\n",
    "        int: The first value of the first row from the result of the SQL query.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the result of the SQL query is not a pandas DataFrame or if it's empty.\n",
    "    \"\"\"\n",
    "    db.execute_update(helpers.get_sql(\n",
    "        \"sql/generate_new_run_id.pysql\",\n",
    "        metadata=json.dumps(metadata),\n",
    "    ))\n",
    "\n",
    "    new_id = db.execute_query(helpers.get_sql(\n",
    "        \"sql/get_new_run_id.pysql\",\n",
    "    ))\n",
    "\n",
    "    return new_id.iloc[0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cde58-429b-49ba-9563-794ee6aefc29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T14:01:29.724582Z",
     "iopub.status.busy": "2023-09-14T14:01:29.723522Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c678fedfb8b64ba2a0060d3030129051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 0.75\n",
    "prior_alpha=1\n",
    "prior_beta=1\n",
    "n_workers = 4\n",
    "scopus_version = 4\n",
    "run_id = get_new_run_id(dict(\n",
    "    threshold=threshold,\n",
    "    prior_alpha=prior_alpha,\n",
    "    prior_beta=prior_beta,\n",
    "    n_workers=n_workers,\n",
    "    scopus_version=scopus_version\n",
    "))\n",
    "\n",
    "for year in tqdm(range(2010, 2023)):\n",
    "    update_year(year, db, scopus_version, run_id, threshold, prior_alpha, prior_beta, n_workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
